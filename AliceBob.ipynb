{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Module, Linear, ReLU, Sigmoid, Sequential, MaxPool2d\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "path_to_save = \"./saved_model\"\n",
    "print(\"GPU:\", GPU_AVAILABLE)\n",
    "\n",
    "def enable_cuda(x):\n",
    "    if GPU_AVAILABLE:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "def to_cpu(x):\n",
    "    if GPU_AVAILABLE:\n",
    "        return x.cpu()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad hoc testing shows good\n",
    "def getLatentState(ram_arr, num_steps, done):\n",
    "    assert(len(ram_arr) == 128)\n",
    "    x_coord = ram_arr[100]\n",
    "    y_coord = ram_arr[102]\n",
    "    prev_score = int(hex(ram_arr[73])[2:]) * 100\n",
    "    prev_score += int(hex(ram_arr[74])[2:])\n",
    "    igloo_blocks = ram_arr[77]\n",
    "    \n",
    "    x_coord -= 16\n",
    "    x_coord /= (160.0 - 16.0)\n",
    "    y_coord -= 22\n",
    "    y_coord /= (140.0 - 22.0)\n",
    "    if igloo_blocks == 255:\n",
    "        igloo_blocks = 0\n",
    "    else:\n",
    "        igloo_blocks += 1\n",
    "    prev_score /= 1600.0\n",
    "    igloo_blocks /= 20.0\n",
    "    num_steps /= 2000.0\n",
    "#     print(f'({x_coord}, {y_coord}), {prev_score}, {igloo_blocks}')\n",
    "    latent = [x_coord, y_coord, prev_score, igloo_blocks, num_steps, int(done)]\n",
    "    latent = np.asarray(latent, dtype=np.float32)\n",
    "    return latent\n",
    "\n",
    "def getRGB(policy, ram_arr, rgb_prev=None):\n",
    "    rgb = env.render(\"rgb_array\")\n",
    "    rgb = policy.preprocess(np.expand_dims(rgb, 0))\n",
    "    if rgb_prev is not None:\n",
    "        rgb_prev[0:-1] = rgb_prev[1:]\n",
    "        rgb_prev[-1] = rgb\n",
    "    else:\n",
    "        rgb_prev = np.concatenate((rgb, rgb, rgb, rgb), axis=0)\n",
    "    return rgb_prev\n",
    "    \n",
    "\n",
    "def latentStateEq(a, b):\n",
    "    eps = 0.05\n",
    "#     print(\" coord:\", a[:2], b[:2], np.linalg.norm(a[:2] - b[:2]) < eps)\n",
    "#     print(\" scores:\", a[2:4], b[2:4], b[2] >= a[2], b[3] >= a[3])\n",
    "    return (np.linalg.norm(a[:2] - b[:2]) < eps and \n",
    "            b[2] >= a[2] and b[3] >= a[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class policy_estimator(Module):   \n",
    "    def __init__(self, env):\n",
    "        super(policy_estimator, self).__init__()\n",
    "        self.input_dim = 6000\n",
    "        self.output_dim = 6\n",
    "        \n",
    "        self.conv_depth = 3\n",
    "        self.pool_stride = 1\n",
    "        hidden_dim = 200\n",
    "        kernel_size = 8\n",
    "\n",
    "        self.conv_layer = Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(16),\n",
    "            ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            ReLU())\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(972, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, self.output_dim))\n",
    "        \n",
    "    # downsample and grayscale\n",
    "    def preprocess(self, I):\n",
    "        x = I[:,45:185,8:]\n",
    "        x = x[:,::2,::2,:]  # downsample by factor of 2.\n",
    "        x = 0.07 * x[:,:,:,2] + 0.72 * x[:,:,:,1] + 0.21 * x[:,:,:,0]\n",
    "#         plt.imshow(x[0], cmap='gray', vmin=0, vmax=255)\n",
    "#         print(x.shape)\n",
    "#         plt.pause(-1)\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "        \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, rgb, latent):\n",
    "#         print(x.shape)\n",
    "        if len(rgb.shape) != 4:# is not None:\n",
    "            rgb = np.asarray(rgb, dtype=np.float32)\n",
    "            rgb = np.expand_dims(rgb, axis=0)\n",
    "            rgb = enable_cuda(torch.FloatTensor(rgb))\n",
    "            latent = np.asarray(latent, dtype=np.float32)\n",
    "            latent = np.expand_dims(latent, axis=0)\n",
    "            latent = enable_cuda(torch.FloatTensor(latent))\n",
    "        x = self.conv_layer(rgb)\n",
    "#         print(\"shape\", x.shape)\n",
    "        x = x.reshape(-1, 960)\n",
    "#         print(x.shape)\n",
    "#         print(latent.shape)\n",
    "        x = torch.cat((x, latent), dim=1)\n",
    "#         print(x.shape)\n",
    "#         x = latent\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    def __init__(self):\n",
    "        self.xexplored = []\n",
    "        self.yexplored = []\n",
    "        \n",
    "    def addEp(self, ep=None):\n",
    "        if ep is None or ep >= len(self.xexplored):\n",
    "            self.xexplored.append([])\n",
    "            self.yexplored.append([])\n",
    "        else:\n",
    "            self.xexplored[ep] = []\n",
    "            self.yexplored[ep] = []\n",
    "    \n",
    "    def addCoord(self, ep, xcoord, ycoord):\n",
    "        self.xexplored[ep].append(ycoord)\n",
    "        self.yexplored[ep].append(1- xcoord)\n",
    "        \n",
    "    def plotEps(self, ep, epEnd=None):\n",
    "        if epEnd is None:\n",
    "            epEnd = ep + 1\n",
    "        flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "        x = flatten(self.xexplored[ep:epEnd])\n",
    "        y = flatten(self.yexplored[ep:epEnd])\n",
    "        heatmap, xedges, yedges = np.histogram2d(x, y, bins=(144//5, 118//5), range=((0,1), (0, 1)))\n",
    "        extent = [0, 1, 0,  1]\n",
    "        \n",
    "        plt.clf()\n",
    "        plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Frostbite-ramDeterministic-v0')\n",
    "env.unwrapped.seed(0)\n",
    "rewards = []\n",
    "s = env.reset()\n",
    "policy = []\n",
    "optimizer = []\n",
    "progress = []\n",
    "\n",
    "for i in range(2):\n",
    "    policy.append(policy_estimator(env))\n",
    "    policy[-1] = enable_cuda(policy[-1])\n",
    "    rgb = getRGB(policy[-1], env.render('rgb_array'))\n",
    "    latent = getLatentState(s, 0, False)\n",
    "    print(\"latent\", latent)\n",
    "    latent = np.concatenate((latent, latent))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        print(policy[-1].forward(rgb, latent))\n",
    "    # Define optimizer\n",
    "    optimizer.append(optim.Adam(policy[-1].parameters(), \n",
    "                           lr=0.0001))\n",
    "    progress.append(ProgressTracker())\n",
    "    rewards.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    rewards = [r if r > 0 else -1 for r in rewards]\n",
    "    r = np.array([(gamma**i) * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    r_std = r.std()\n",
    "    eps = 0.00001 \n",
    "    if (r_std < eps):\n",
    "#         assert(False)\n",
    "        pass\n",
    "    return r - r.mean()#/r_std\n",
    "\n",
    "def get_action(env, policy, s_0, debug=False):\n",
    "        # Get actions and convert to numpy array\n",
    "    with torch.no_grad():\n",
    "        action_probs = to_cpu(policy(*s_0).detach())\n",
    "        if (torch.isnan(action_probs).any()):\n",
    "            print(action_probs)\n",
    "            assert(False)\n",
    "\n",
    "    if debug:\n",
    "        print(\" logits:\", action_probs)\n",
    "    action_probs = torch.distributions.Categorical(logits=action_probs)\n",
    "    action = action_probs.sample()\n",
    "    return action, action_probs.probs[0, action]\n",
    "\n",
    "def finish_batch(policy, optimizer, all_rstates, all_lstates, all_actions,\n",
    "                     all_actions_probs, all_rewards, eps_clip=0.1):\n",
    "    n_batch = min(len(all_actions), 24325)\n",
    "    idxs = random.sample(range(len(all_actions)), n_batch)\n",
    "    rstate_batch = enable_cuda(torch.FloatTensor([all_rstates[i] for i in idxs]))\n",
    "#     rstate_batch = None\n",
    "    lstate_batch = enable_cuda(torch.FloatTensor([all_lstates[i] for i in idxs]))\n",
    "    action_batch = np.array([all_actions[i] for i in idxs], np.uint8)\n",
    "    old_probs_batch = enable_cuda(torch.cat([all_actions_probs[i] for i in idxs]))\n",
    "    advantage_batch = enable_cuda(torch.FloatTensor([all_rewards[i] for i in idxs]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "\n",
    "    # n_batch x output_dim\n",
    "    new_probs_batch = policy(rstate_batch, lstate_batch)\n",
    "    new_probs_sel = np.identity(policy.output_dim)[action_batch]\n",
    "\n",
    "    new_probs_sel = enable_cuda(torch.FloatTensor(new_probs_sel))\n",
    "    new_probs_batch = torch.sum(F.softmax(new_probs_batch, dim=1) * new_probs_sel, dim=1)\n",
    "\n",
    "#     print(\"batch \", new_probs_batch)\n",
    "    if (torch.isnan(new_probs_batch).any()):\n",
    "        print(new_probs_batch)\n",
    "        assert(False)\n",
    "    if (torch.isnan(old_probs_batch).any()):\n",
    "        print(old_probs_batch)\n",
    "        assert(False)\n",
    "    if (torch.isnan(advantage_batch).any()):\n",
    "        print(advantage_batch)\n",
    "        assert(False)\n",
    "\n",
    "    r = new_probs_batch / old_probs_batch\n",
    "    loss1 = r * advantage_batch\n",
    "    loss2 = torch.clamp(r, 1-eps_clip, 1+eps_clip) * advantage_batch\n",
    "    loss = torch.min(loss1, loss2)\n",
    "    loss = torch.mean(loss)\n",
    "    print(\"Loss \", loss)\n",
    "\n",
    "    # Calculate gradients\n",
    "    loss.backward()\n",
    "    # Apply gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kStopAction = 1\n",
    "kNoop = 0\n",
    "def selfplayEpisode(env, policy_a, policy_b, progress_a, progress_b,\n",
    "                        tmax = 30, tdiff = 10, scale=1.0, gamma=0.99):\n",
    "    ta, tb = 0, 0\n",
    "    obs = env.reset()\n",
    "    s0_latent = getLatentState(obs, 0, False)\n",
    "    s0_rgb = getRGB(policy_a, env.render('rgb_array'))\n",
    "    sa_rgb, sb_rgb = s0_rgb, s0_rgb\n",
    "    done = False\n",
    "    ra_game = 0\n",
    "    rb_game = 0\n",
    "    sra_arr, sla_arr, srb_arr, slb_arr = [], [], [], []\n",
    "    aa_arr, apa_arr = [], []\n",
    "    ab_arr, apb_arr = [], []\n",
    "    progress_a.addEp()\n",
    "    progress_b.addEp()\n",
    "    while ta < tmax:\n",
    "        sa_rgb = getRGB(policy_a, env.render('rgb_array'), sa_rgb)\n",
    "        sa_latent = getLatentState(obs, ta, done) # don't clobber for bob\n",
    "        progress_a.addCoord(-1, sa_latent[0], sa_latent[1])\n",
    "        sa_latent_ = np.concatenate((sa_latent, s0_latent))\n",
    "\n",
    "        sa = [sa_rgb, sa_latent_]\n",
    "        action, action_prob = get_action(env, policy_a, sa)\n",
    "        sra_arr.append(sa_rgb)\n",
    "        sla_arr.append(sa_latent_)\n",
    "        aa_arr.append(action)\n",
    "        apa_arr.append(action_prob)\n",
    "        ta += 1\n",
    "        if action == kStopAction:\n",
    "            break\n",
    "        if not done:\n",
    "            obs, reward, done, lives = env.step(action)\n",
    "            if lives['ale.lives'] < 4:\n",
    "                done = True\n",
    "            ra_game += reward\n",
    "\n",
    "    done = False\n",
    "    obs = env.reset()\n",
    "    while tb < tmax + tdiff//2:\n",
    "        sb_rgb = getRGB(policy_b, env.render('rgb_array'), sb_rgb)\n",
    "        sb_latent = getLatentState(obs, tb, done)\n",
    "        progress_b.addCoord(-1, sb_latent[0], sb_latent[1])\n",
    "        sb_latent_ = np.concatenate((sb_latent, sa_latent))\n",
    "\n",
    "        sb = [sb_rgb, sb_latent_]\n",
    "        action, action_prob = get_action(env, policy_b, sb)\n",
    "        srb_arr.append(sb_rgb)\n",
    "        slb_arr.append(sb_latent_)\n",
    "        ab_arr.append(action)\n",
    "        apb_arr.append(action_prob)\n",
    "        tb += 1\n",
    "        if latentStateEq(sa_latent, sb_latent):\n",
    "            print(\" Bob got it! \", tb, ta)\n",
    "            break\n",
    "        if done:\n",
    "            tb = ta + tdiff\n",
    "            break\n",
    "        # translate stop to noop\n",
    "        action = kNoop if action == kStopAction else action\n",
    "        obs, reward, done, lives = env.step(action)\n",
    "        if lives['ale.lives'] < 4:\n",
    "            done = True\n",
    "        rb_game += reward\n",
    "        \n",
    "        \n",
    "    print(\"  > game rewards: \", ra_game, rb_game)\n",
    "    ra = (1 - scale) * ra_game + scale * max(0, tb - ta)\n",
    "    rb = (1 - scale) * rb_game + -scale * tb\n",
    "#     print(\"  > self play: \", ra, rb)\n",
    "    # update with custom losses\n",
    "    dra_arr = discount_rewards([0.0 if i!=(ta - 1) else ra for i in range(ta)], gamma)\n",
    "    drb_arr = discount_rewards([0.0 if i!=(tb - 1) else rb for i in range(tb)], gamma)\n",
    "    \n",
    "    return ((sra_arr, sla_arr, aa_arr, apa_arr, dra_arr, ra),\n",
    "            (srb_arr, slb_arr, ab_arr, apb_arr, drb_arr, rb))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reinforce(env, po_a, po_b, o_a, o_b, pr_a, pr_b, num_episodes=120,\n",
    "              batch_size=11, max_steps =5000, gamma=0.99, eps_clip=0.1):\n",
    "\n",
    "    # Set up lists to hold results\n",
    "    # alice @ index 0, bob @ 1\n",
    "    dr, tr = [[],[]], [[], []]  # discounted, total rewards\n",
    "    a, ap = [[], []], [[], []]  # action,     action prob\n",
    "    sr, sl = [[], []], [[], []] # state rgb,  latent\n",
    "    batch_counter = 1\n",
    "    \n",
    "    \n",
    "#     action_space = np.arange(env.action_space.n)\n",
    "    try:\n",
    "        for ep in range(num_episodes):\n",
    "            res = selfplayEpisode(env, po_a, po_b, pr_a, pr_b)\n",
    "            for i in range(2):\n",
    "                sr[i].extend(res[i][0])\n",
    "                sl[i].extend(res[i][1])\n",
    "                a[i].extend(res[i][2])\n",
    "                ap[i].extend(res[i][3])\n",
    "                dr[i].extend(res[i][4])\n",
    "                tr[i].append(res[i][5])\n",
    "\n",
    "            batch_counter += 1\n",
    "            print(\"Ep \", ep, \", len \", len(res[0][2]), len(res[1][2]), \", aa:\", a[0][-1], a[1][-1], \n",
    "                      \" ap[0]:\", ap[0][-1], ap[1][-1],\" tr:\", tr[0][-1],tr[1][-1])\n",
    "\n",
    "            # If batch is complete, update network\n",
    "            if batch_counter >= batch_size:\n",
    "                for _ in range(2):#batch_size//2):\n",
    "                    for i in range(2):\n",
    "                        if i == 0:\n",
    "                            finish_batch(po_a, o_a, sr[i], sl[i], a[i], ap[i], dr[i])\n",
    "                        if i == 1:\n",
    "                            finish_batch(po_b, o_b, sr[i], sl[i], a[i], ap[i], dr[i])\n",
    "\n",
    "                for i in range(2):\n",
    "                    sr[i].clear()\n",
    "                    sl[i].clear()\n",
    "                    a[i].clear()\n",
    "                    ap[i].clear()\n",
    "                    dr[i].clear()\n",
    "#                     tr[i].clear()\n",
    "                batch_counter = 1\n",
    "\n",
    "                # Print running average\n",
    "                print(\"Ep: {} Average of last 10: {:.2f} {:.2f}\\n\".format(\n",
    "                    ep + 1, np.mean(tr[0][-10:]), np.mean(tr[0][-10:])), end=\"\")\n",
    "\n",
    "    #             assert(False)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e\n",
    "                \n",
    "    return tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "rewards_ = reinforce(env, policy[0], policy[1], optimizer[0], optimizer[1], progress[0], progress[1], 240)\n",
    "rewards[0].extend(rewards_[0])\n",
    "rewards[1].extend(rewards_[1])\n",
    "window = 10\n",
    "plt.figure(figsize=(12,8))\n",
    "smoothed_rewards = []\n",
    "for ii in range(2):\n",
    "    smoothed_rewards.append([np.mean(rewards[ii][i-window:i+1]) if i > window \n",
    "                        else np.mean(rewards[ii][:i+1]) for i in range(len(rewards[ii]) - 100)])\n",
    "    name = \"Alice\" if ii == 0 else \"Bob\"\n",
    "    plt.plot(rewards[ii][:-100], label = name)\n",
    "    plt.plot(smoothed_rewards[ii], label = name + \"_smooth\")\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.legend()\n",
    "plt.title(\"Alice-Bob style training with fixed Tmax\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress[1].plotEps(0,750)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in pe.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
