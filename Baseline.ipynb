{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import sys\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import Module, Linear, ReLU, Sigmoid, Sequential, MaxPool2d\n",
    "from torch.nn import functional as F\n",
    "\n",
    "print(sys.version)\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GPU_AVAILABLE = torch.cuda.is_available()\n",
    "path_to_save = \"./saved_model\"\n",
    "print(\"GPU:\", GPU_AVAILABLE)\n",
    "\n",
    "def enable_cuda(x):\n",
    "    if GPU_AVAILABLE:\n",
    "        return x.cuda()\n",
    "    return x\n",
    "def to_cpu(x):\n",
    "    if GPU_AVAILABLE:\n",
    "        return x.cpu()\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ad hoc testing shows good\n",
    "def getLatentState(ram_arr, num_steps):\n",
    "    assert(len(ram_arr) == 128)\n",
    "    x_coord = ram_arr[100]\n",
    "    y_coord = ram_arr[102]\n",
    "    prev_score = int(hex(ram_arr[73])[2:]) * 100\n",
    "    prev_score += int(hex(ram_arr[74])[2:])\n",
    "    igloo_blocks = ram_arr[77]\n",
    "    \n",
    "    x_coord -= 16\n",
    "    x_coord /= (160.0 - 16.0)\n",
    "    y_coord -= 22\n",
    "    y_coord /= (140.0 - 22.0)\n",
    "    if igloo_blocks == 255:\n",
    "        igloo_blocks = 0\n",
    "    else:\n",
    "        igloo_blocks += 1\n",
    "    prev_score /= 1600.0\n",
    "    igloo_blocks /= 20.0\n",
    "    num_steps /= 2000.0\n",
    "    \n",
    "    return [x_coord, y_coord, prev_score, igloo_blocks, num_steps]\n",
    "\n",
    "def getState(policy, num_steps, ram_arr, rgb_prev=None):\n",
    "    latent = getLatentState(ram_arr, num_steps)\n",
    "    latent = np.asarray(latent, dtype=np.float32)\n",
    "    rgb = env.render(\"rgb_array\")\n",
    "    rgb = policy.preprocess(np.expand_dims(rgb, 0))\n",
    "    if rgb_prev is not None:\n",
    "        rgb_prev[0:-1] = rgb_prev[1:]\n",
    "        rgb_prev[-1] = rgb\n",
    "    else:\n",
    "        rgb_prev = np.concatenate((rgb, rgb, rgb, rgb), axis=0)\n",
    "        \n",
    "    return (rgb_prev, latent)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class policy_estimator(Module):   \n",
    "    def __init__(self, env):\n",
    "        super(policy_estimator, self).__init__()\n",
    "        _ = env.observation_space.shape\n",
    "        self.input_dim = 6000\n",
    "        self.output_dim = 6\n",
    "        \n",
    "        self.conv_depth = 3\n",
    "        self.pool_stride = 1\n",
    "        hidden_dim = 200\n",
    "        kernel_size = 8\n",
    "\n",
    "        self.conv_layer = Sequential(\n",
    "            nn.Conv2d(in_channels=4, out_channels=16, kernel_size=8, stride=4),\n",
    "            nn.BatchNorm2d(16),\n",
    "            ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=4, stride=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            ReLU())\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(965, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, self.output_dim))\n",
    "        \n",
    "    # downsample and grayscale\n",
    "    def preprocess(self, I):\n",
    "        x = I[:,45:185,8:]\n",
    "        x = x[:,::2,::2,:]  # downsample by factor of 2.\n",
    "        x = 0.07 * x[:,:,:,2] + 0.72 * x[:,:,:,1] + 0.21 * x[:,:,:,0]\n",
    "        x = x.astype(np.float32)\n",
    "        return x\n",
    "        \n",
    "    # Defining the forward pass    \n",
    "    def forward(self, rgb, latent):\n",
    "#         print(x.shape)\n",
    "        if len(rgb.shape) != 4:# is not None:\n",
    "            rgb = np.asarray(rgb, dtype=np.float32)\n",
    "            rgb = np.expand_dims(rgb, axis=0)\n",
    "            rgb = enable_cuda(torch.FloatTensor(rgb))\n",
    "            latent = np.asarray(latent, dtype=np.float32)\n",
    "            latent = np.expand_dims(latent, axis=0)\n",
    "            latent = enable_cuda(torch.FloatTensor(latent))\n",
    "        x = self.conv_layer(rgb)\n",
    "#         print(\"shape\", x.shape)\n",
    "        x = x.reshape(-1, 960)\n",
    "#         print(x.shape)\n",
    "#         print(latent.shape)\n",
    "        x = torch.cat((x, latent), dim=1)\n",
    "#         print(x.shape)\n",
    "#         x = latent\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Frostbite-ramDeterministic-v0')\n",
    "env.unwrapped.seed(0)\n",
    "rewards = []\n",
    "s = env.reset()\n",
    "# print(s)\n",
    "print(s.shape)\n",
    "print(env.observation_space.shape)\n",
    "pe = policy_estimator(env)\n",
    "pe = enable_cuda(pe)\n",
    "(rgb, latent) = getState( pe, 0, s, None)\n",
    "print(\"latent\", latent)\n",
    "\n",
    "print(pe.forward(rgb, latent))\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(pe.parameters(), \n",
    "                       lr=0.0001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProgressTracker:\n",
    "    def __init__(self):\n",
    "        self.xexplored = []\n",
    "        self.yexplored = []\n",
    "        \n",
    "    def addEp(self, ep=None):\n",
    "        if ep is None or ep >= len(self.xexplored):\n",
    "            self.xexplored.append([])\n",
    "            self.yexplored.append([])\n",
    "        else:\n",
    "            self.xexplored[ep] = []\n",
    "            self.yexplored[ep] = []\n",
    "    \n",
    "    def addCoord(self, ep, xcoord, ycoord):\n",
    "        self.xexplored[ep].append(ycoord)\n",
    "        self.yexplored[ep].append(1- xcoord)\n",
    "        \n",
    "    def plotEps(self, ep, epEnd=None):\n",
    "        if epEnd is None:\n",
    "            epEnd = ep + 1\n",
    "        flatten = lambda t: [item for sublist in t for item in sublist]\n",
    "        x = flatten(self.xexplored[ep:epEnd])\n",
    "        y = flatten(self.yexplored[ep:epEnd])\n",
    "        heatmap, xedges, yedges = np.histogram2d(x, y, bins=(144//5, 118//5), range=((0,1), (0, 1)))\n",
    "        extent = [0, 1, 0,  1]\n",
    "        \n",
    "        plt.clf()\n",
    "        plt.imshow(heatmap.T, extent=extent, origin='lower')\n",
    "        plt.show()\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "progress = ProgressTracker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    rewards = [r if r > 0 else -1 for r in rewards]\n",
    "    r = np.array([(gamma**i) * rewards[i] \n",
    "                  for i in range(len(rewards))])\n",
    "    # Reverse the array direction for cumsum and then\n",
    "    # revert back to the original order\n",
    "    r = r[::-1].cumsum()[::-1]\n",
    "    r_std = r.std()\n",
    "    eps = 0.0001 \n",
    "#     if (r_std < eps):\n",
    "#         return r - r.mean()\n",
    "    return r - r.mean()#/r_std\n",
    "\n",
    "def get_action(env, policy, s_0, debug=False):\n",
    "        # Get actions and convert to numpy array\n",
    "    with torch.no_grad():\n",
    "        action_probs = to_cpu(policy(*s_0).detach())\n",
    "        if (torch.isnan(action_probs).any()):\n",
    "            print(action_probs)\n",
    "            assert(False)\n",
    "\n",
    "    if debug:\n",
    "        print(\" logits:\", action_probs)\n",
    "    action_probs = torch.distributions.Categorical(logits=action_probs)\n",
    "    action = action_probs.sample()\n",
    "    return action, action_probs.probs[0, action]\n",
    "\n",
    "def do_episode(env, policy, progress, ep_num, gamma, max_steps):\n",
    "    ram_0 = env.reset()\n",
    "    progress.addEp()\n",
    "    rgb_0 = None\n",
    "\n",
    "    rstates = []\n",
    "    lstates = []\n",
    "    rewards = []\n",
    "    actions = []\n",
    "    actions_probs = []\n",
    "    complete = False\n",
    "    while complete == False and len(rstates) < max_steps:\n",
    "        # Get actions and convert to numpy array\n",
    "        rgb_0, latent = getState(pe, len(rstates), ram_0, rgb_0)\n",
    "        \n",
    "        progress.addCoord(-1, latent[0], latent[1])\n",
    "        action, prob = get_action(env, policy, (rgb_0, latent), len(rstates) == 2)\n",
    "        ram_0, r, complete, lives = env.step(action)\n",
    "        if lives['ale.lives'] < 4:\n",
    "            complete = True\n",
    "\n",
    "        rstates.append(rgb_0)\n",
    "        lstates.append(latent)\n",
    "        rewards.append(r)\n",
    "        actions.append(action)\n",
    "        total_reward = sum(rewards)\n",
    "        drewards = discount_rewards(rewards, gamma)\n",
    "        actions_probs.append(prob)\n",
    "    return rstates, lstates, actions, actions_probs, total_reward, drewards\n",
    "\n",
    "def finish_batch(policy, optimizer, all_rstates, all_lstates, all_actions,\n",
    "                     all_actions_probs, all_rewards, eps_clip=0.1):\n",
    "    n_batch = min(len(all_actions), 24325)\n",
    "    idxs = random.sample(range(len(all_actions)), n_batch)\n",
    "    rstate_batch = enable_cuda(torch.FloatTensor([all_rstates[i] for i in idxs]))\n",
    "    lstate_batch = enable_cuda(torch.FloatTensor([all_lstates[i] for i in idxs]))\n",
    "    action_batch = np.array([all_actions[i] for i in idxs], np.uint8)\n",
    "    old_probs_batch = enable_cuda(torch.cat([all_actions_probs[i] for i in idxs]))\n",
    "    advantage_batch = enable_cuda(torch.FloatTensor([all_rewards[i] for i in idxs]))\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # n_batch x output_dim\n",
    "    new_probs_batch = pe(rstate_batch, lstate_batch)\n",
    "    new_probs_sel = np.identity(pe.output_dim)[action_batch]\n",
    "\n",
    "    new_probs_sel = enable_cuda(torch.FloatTensor(new_probs_sel))\n",
    "    new_probs_batch = torch.sum(F.softmax(new_probs_batch, dim=1) * new_probs_sel, dim=1)\n",
    "\n",
    "#     print(\"batch \", new_probs_batch)\n",
    "    if (torch.isnan(new_probs_batch).any()):\n",
    "        print(new_probs_batch)\n",
    "        assert(False)\n",
    "    if (torch.isnan(old_probs_batch).any()):\n",
    "        print(old_probs_batch)\n",
    "        assert(False)\n",
    "    if (torch.isnan(advantage_batch).any()):\n",
    "        print(advantage_batch)\n",
    "        assert(False)\n",
    "\n",
    "    r = new_probs_batch / old_probs_batch\n",
    "    loss1 = r * advantage_batch\n",
    "    loss2 = torch.clamp(r, 1-eps_clip, 1+eps_clip) * advantage_batch\n",
    "    loss = torch.min(loss1, loss2)\n",
    "    loss = torch.mean(loss)\n",
    "    print(\"Loss \", loss)\n",
    "\n",
    "    # Calculate gradients\n",
    "    loss.backward()\n",
    "    # Apply gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(env, policy, optimizer, progress, num_episodes=120,\n",
    "              batch_size=11, max_steps =5000, gamma=0.99, eps_clip=0.1):\n",
    "\n",
    "    # Set up lists to hold results\n",
    "    total_rewards = []\n",
    "    all_rewards = []\n",
    "    all_actions = []\n",
    "    all_actions_probs = []\n",
    "    all_rstates = []\n",
    "    all_lstates = []\n",
    "    batch_counter = 1\n",
    "    \n",
    "    \n",
    "#     action_space = np.arange(env.action_space.n)\n",
    "    try:\n",
    "        for ep in range(num_episodes):\n",
    "            rs, ls, a, a_prob, tot_r, dis_r = do_episode(env, policy, progress, ep, gamma, max_steps)\n",
    "\n",
    "            # If complete, batch data\n",
    "            all_rewards.extend(dis_r)\n",
    "            all_rstates.extend(rs)\n",
    "            all_lstates.extend(ls)\n",
    "            all_actions_probs.extend(a_prob)\n",
    "            all_actions.extend(a)\n",
    "            batch_counter += 1\n",
    "            total_rewards.append(tot_r)\n",
    "            print(\"Ep \", ep, \", len \", len(a),\n",
    "                      \" last_action:\", a[-1], \" last_action_prob:\", a_prob[-1],\" reward:\", tot_r)\n",
    "\n",
    "            # If batch is complete, update network\n",
    "            if batch_counter >= batch_size:\n",
    "                for _ in range(2):\n",
    "                    finish_batch(pe, optimizer, all_rstates, all_lstates, all_actions,\n",
    "                         all_actions_probs, all_rewards)\n",
    "\n",
    "                all_rewards = []\n",
    "                all_actions_probs = []\n",
    "                all_actions = []\n",
    "                all_rstates = []\n",
    "                all_lstates = []\n",
    "                batch_counter = 1\n",
    "\n",
    "                # Print running average\n",
    "                print(\"Ep: {} Average of last 10: {:.2f}\\n\".format(\n",
    "                    ep + 1, np.mean(total_rewards[-10:])), end=\"\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        raise e\n",
    "                \n",
    "    return total_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define optimizer\n",
    "rewards_ = reinforce(env, pe, optimizer, progress, 200)\n",
    "rewards.extend(rewards_)\n",
    "window = 10\n",
    "smoothed_rewards = [np.mean(rewards[i-window:i+1]) if i > window \n",
    "                    else np.mean(rewards[:i+1]) for i in range(len(rewards))]\n",
    "\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(rewards)\n",
    "plt.plot(smoothed_rewards)\n",
    "plt.ylabel('Total Rewards')\n",
    "plt.xlabel('Episodes')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "progress.plotEps(900,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "ram_0 = env.reset()\n",
    "rgb_0 = None\n",
    "i = 0\n",
    "complete = False\n",
    "while complete == False and i < 500:\n",
    "    # Get actions and convert to numpy array\n",
    "    rgb_0, latent = getState(pe, i, ram_0, rgb_0)\n",
    "    i += 1\n",
    "#         print(i)\n",
    "\n",
    "    action, prob = get_action(env, pe, (rgb_0, latent), i == 2)\n",
    "    \n",
    "    plt.figure()\n",
    "    plt.imshow(env.render(\"rgb_array\"))\n",
    "    plt.show()\n",
    "    ram_0, r, complete, lives = env.step(action)\n",
    "#     env.render()\n",
    "    if r > 0:\n",
    "        print(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in pe.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
